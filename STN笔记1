1、深度卷积神经网络模型虽然已经在很多领域中取得了较好的效果，但大多是依赖于海量的数据训练实现的，但这些模型依旧十分脆弱。例如，对一张图进行平移、旋转和缩放等操作后，会使原有的模型识别准确度下降，该现象可以理解为深度卷积神经网络的一个通病，尤其在训练样本不足的情况下，VGG16、ResNet50、Inception等经典的模型也会出现类似问题。

可以从两方面入手：

（1）从样本多样性入手，将样本进行更多的变化，令模型见多识广，数据增强。
（2）从样本预处理入手，采用仿射变换对现有的图片进行修正，该方法的思想是将图片按照某一规格进行统一修正。


2、空间变换网络（Spatial Transformer Network，STN）模型是在仿射变换领域最基础的文字识别模型之一。该模型的功能是在训练过程中学习对原始图片进行平移、缩放、旋转等变换的参数，将传入图片变成统一的格式，以便模型更好的识别。

STN模型不仅可以应用在OCR领域，还可以应用在人脸识别任务中，以及任何需要图像矫正的场景。例如：对检测的人脸图片进行关键点检测，然后利用STN模型对图片进行仿射变换，使关键点对齐，最后进行人脸配准。


3、STN模型由3部分组成：

（1）仿射参数：一般由一个全连接网络实现，该网络最终输出6个数值，将其变为2*3矩阵，其中每两个为一组，分别代表仿射变换中平移、旋转、缩放所对应的参数。

（2）坐标映射：创建一个与输出图片大小相同的矩阵，把该矩阵与仿射矩阵参数相乘，把所得结果当作目标图片中每个像素点对应原图的坐标。

（3）采样器：使用坐标映射部分中每个像素值的坐标值，在原始图片上取对应的像素，并将其填充到目标图片中。



4、在对图片进行旋转的时候，默认是将坐标原点放置在了图片正中心的位置，也正因为这样，才会把x方向、y方向的起始和终止都设置为-1和1，进行标准的数学旋转。

我们可以手动输入仿射参数对图片进行仿射变换，进而验证stn_transformer类是否可以正常工作。

代码定义了一个权重为0，偏置值为[[0.5, 0, 0.1], [0, 0.5. -0.5]]的全连接层，该网络层的输出结果与偏置值一致。
这里就是神经网络自适应阈值的妙用了！！！借助神经网络的多个连接权重参数w，会输出我们想要的形变参数阈值，这些阈值并不是固定的，每张图片都会是不一样的，而借助有监督学习的方式，总可以让它根据图片内容学习出我们想要的效果！！！！！！

代码中使用全连接层的目的是，模拟stn_transformer类在神经网络中的嵌入用法。如果仅仅用于测试stn_transformer类，可以直接为locnet赋值，使其等于[[0.5, 0, 0.1], [0, 0.5. -0.5]]。这一行代码使用全0变量只是为了模拟stn层在嵌入网络中的上下层关系。



5、深入理解代码中的仿射变换参数含义：

仿射变换矩阵： [[a11, a12, a13], [a21, a22, a23]]，原始坐标 (x, y, 1)

新坐标： (new_x, new_y).T = [[a11, a12, a13], [a21, a22, a23]] * (x, y, 1).T

也就是说：

new_x = a11 * x + a12 * y + a13
new_y = a21 * x + a22 * y + a23

所以对于这里的[[0.5, 0, 0.1], [0, 0.5. -0.5]]，第3列 0.1、-0.5表示的是平移参数，而第1-第2列表示的是旋转放缩参数。

第1列-第2列的四个数值，综合融合了旋转和放缩两个变换的信息，并不能直接从 数学公式中简单分类出来。
数值0-1之间表示将原图放大，数值大于1时表示将原图缩小。



6、独立组建层，Independent Component， IC层。可以理解为将BN层与Dropout层组合起来，对每层的输入数据进行处理，可以更大限度地减小当前层对上一层输出样本的分布依赖，使得每层的数据处理更加独立。

使用IC层的网络模型，直接将调整分布的代码(BN部分)放到网络的输入端，不需要再考虑与激活函数的前后位置关系。原始结构中的"激活函数和BN"部分被拆开，变成独立的"激活函数"，而"BN"部分被融合到改进后的结构"IC"层，并放在了网络层的后面。

有关IC层的论文，可以在arXiv网站搜索论文编号"1905.05928"。



7、这个stn层的自适应思路真的是太强了！！！！！！

仿射变换参数不是固定不变的，是基于图片整体特征再网络加权得到的。对于一张图片，需要先利用卷积池化提取图片的各种特征，基于这个特征，学会这张图片特定的自适应仿射变换参数，然后用这些参数来调整原图，再重新进行图片分类。

表面上是自适应学会阈值，实际上模型的处理思路和人是一样的，先看清图片整体内容，再自行设置合适的阈值。


8、代码对stn模型进行了手动初始化，该仿射参数的意义是不对图片进行任何改变。

这里的手动初始化并不是必要的，如果使用默认的初始化模型也可以收敛，但得到的仿射变换区域是扭曲、翻转的。

GlobalAveragePooling2D()函数会自动匹配输入特征图的尺寸，用与输入特征图尺寸相同的滤波器进行全局最优化操作，这样不需要首动计算和填入输入特征的尺寸，而AveragePooling2D()函数就需要手动指定滤波器的大小。


9、经过实验验证，虽然加入stn之后稍微难训练了一些，但在长时间训练之后依然能取得不错的分类效果。

大概在训练10个epoch之后，就能达到分类精度90%的效果。我总共训练了400轮，最后分类精度达到99.4%，比较满意。
我同时也检查了一下误判情况，发现要么是字符颜色过淡，人眼也难以辨别；要么就是D和O和Q之类的辨别错误，情有可原。

接着我可视化了一下stn层对原始图片的形变效果，的确有效！！！
最主要的变化在于两点：一是相应字符区域经过stn层调整之后都变大了，二是原本字符所在偏僻位置经过调整后都渐近居中了。
的确有效果！！！



10、最后，我想验证一个想法：

目前大家都知道，CNN拿来做图片分类无论怎么搭建模型效果都会很好。但对于目前这种字符偏移和大小尺寸不一致的图片分类问题，CNN还能取得之前那种良好的分类效果吗？？？而stn层对最终算法精度又有没有提升呢？

如果没有提升，那我还白费心思加上仿射变换stn层干什么？？？


加入stn层之后果然会比较难以训练，单纯cnn的收敛速度比stn-cnn快上不少，在第6轮的时候就有0.8的预测精度了，而stn-cnn在第8轮才能达到0.8的预测精度。

最终结果显示，在相同的特征提取、相同的优化器和epoch下，普通cnn训练400轮之后精度最高只能达到98.6%，几乎难以达到99%，这说明stn-cnn比起cnn，对于这种不规则图片分类的精度多了1%的点，说明stn-cnn进行的图片矫正的确是有效果的。



11、其实这段代码我还有一个提升算法效果的好办法，就是对训练数据做预处理：

不输入彩色rgb图片做分类，只处理灰度图，并借助pixel-min、pixel-max先归一化，网络学起来保证更加轻松简单。


12、  

# 我发现这个stn网络的内在逻辑，无非是借助6个参数，自己手动实现了一遍图片仿射变换的原理。
# 而这6个参数，我们可以自己设定，也可以借助神经网络来拟合这6个参数，从而借助神经网络大量计算自动实现仿射变换。

# 这是我想出来了一个极佳的发paper思路和创新点：
# 在opencv中，往往有些阈值难以设定，我们经常在根据每张图片的内容来调整这些阈值
# 那么，借助stn网络的思想，我可不可以把这些阈值设置成神经网络层中的某些位置参数，让网络来替我们自适应调整呢？？？
# 这个自适应思路太棒了！！！完全就是人思考问题的方式，先根据图片内容，来自适应调整阈值！！！！！
# 这个科研思路比以往所有我想到过的思路都棒！！！！！！
# 这种研究思路直接对整个opencv函数都是一个巨大的图片，一种巨大的变革。
# 而且这直接解决了一种如何创新网络层的思路，缺什么功能就把这个功能定义成网络层，需要的参数直接用全连接神经元表示。



