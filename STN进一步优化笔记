1、STN层的输入参数locnet是由全连接层生成的，该全连接层没有使用激活函数，这就会导致参数locnet的值域没有得到控制。在训练过程中，如果该参数产生了一个异常值（例如：较大的平移值、较大的缩放值、较小的缩放值），则会导致经过STN变换后的图片出现严重的信息缺失，这会导致模型无法收敛。

所以我对模型做了修正： 在全连接层添加tanh激活函数。

loc_net = Dense(6, kernel_initializer='zeros', activation='tanh',
                    bias_initializer=tf.keras.initializers.constant([[1.0, 0, 0], [0, 1.0, 0]]))(loc_net)

在全连接层添加激活函数，对仿射变换参数locnet的值域进行限制。tanh激活函数值域为[-1, 1]，它与仿射变换参数的值域相匹配。




2、广义的STN：

如果对STN层进行进一步的抽象，则它可以应用得更广泛。不仅可以使用STN对输入图片进行仿射变换，还可以将CNN的输出特征当作一幅图像，用STN进行仿射变换。

将STN广义化后，可以用于神经网络的任意两层之间，对特征数据进行调整，从而使其表达得更准确。

这里STN层相当于对feature map进行仿射变换，原始输入是(None, 5, 5, 64)，仿射变换输出是(None, 5, 5, 64)，保持了尺寸相一致，其实也就相当于把feature map也理解成一种广义图像。这样就节省了一次卷积特征提取过程。

因为CNNSTNRNN模型比STNCRNN模型少调用一次图片特征处理层Feature Extractor，所以模型更容易逊俩，收敛会更快。



3、可惜。STN这个想法虽然很惊艳，而且广义STN更具备一种高层数学上的抽象拓广，但可以STN对实际用处提升并不是很大，单纯通过数据扩增的效果已经足以实现不同角度的图像处理。


4、经过实验发现：

（1）对于添加了激活函数的tanh_stn_model，训练效果和之前没加激活函数差不多，两者效果都很好，训练400个epoch后最佳权重也可以达到99.7%的精度。

（2）对于广义general_stn_model，可能是特征提取不够好，我们在提取到的feature map直接做仿射变换，后面直接拉直进行分类了，少了后续的细化操作，训练400个epoch最佳权重只能达到98%左右。
